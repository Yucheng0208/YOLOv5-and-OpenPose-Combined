![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.001.png)

See discussions, stats, and author profiles for this publication at:[ https://www.researchgate.net/publication/372240930](https://www.researchgate.net/publication/372240930_Based-on_Computer_Vision_Applications_for_Bus_Stop_Passenger_Detection_System?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_2&_esc=publicationCoverPdf)

[Based-on Computer Vision Applications for Bus Stop Passenger Detection System](https://www.researchgate.net/publication/372240930_Based-on_Computer_Vision_Applications_for_Bus_Stop_Passenger_Detection_System?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_3&_esc=publicationCoverPdf)

**Conference Paper** ¬∑ April 2023

DOI: 10.1109/ICEIB57887.2023.10169919![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.002.png)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.003.png)

CITATION READ

1 1

**4 authors**, including:

[Yu-Cheng Chang](https://www.researchgate.net/profile/Yu-Cheng-Chang-14?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_5&_esc=publicationCoverPdf)[ ](https://www.researchgate.net/profile/Hua-Wen-Tsai?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_4&_esc=publicationCoverPdf)[Hua-Wen Tsai](https://www.researchgate.net/profile/Hua-Wen-Tsai?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_5&_esc=publicationCoverPdf)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.004.png)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.005.png)

[Lunghwa University of Science and Technology](https://www.researchgate.net/institution/Lunghwa-University-of-Science-and-Technology?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_6&_esc=publicationCoverPdf)[ ](https://www.researchgate.net/profile/Hua-Wen-Tsai?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_4&_esc=publicationCoverPdf)[Lunghwa University of Science and Technology ](https://www.researchgate.net/institution/Lunghwa-University-of-Science-and-Technology?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_6&_esc=publicationCoverPdf)**2** PUBLICATIONS **1** CITATION **22** PUBLICATIONS **438** CITATIONS

[SEE PROFILE](https://www.researchgate.net/profile/Yu-Cheng-Chang-14?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_7&_esc=publicationCoverPdf) [SEE PROFILE](https://www.researchgate.net/profile/Hua-Wen-Tsai?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.006.png)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.007.png)

[Zong-Rong Wu](https://www.researchgate.net/profile/Zong-Rong-Wu?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_5&_esc=publicationCoverPdf)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.008.png)

[National Yunlin University of Science and Technology ](https://www.researchgate.net/institution/National-Yunlin-University-of-Science-and-Technology?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_6&_esc=publicationCoverPdf)**1** PUBLICATION **1** CITATION

[SEE PROFILE](https://www.researchgate.net/profile/Zong-Rong-Wu?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.009.png)

All content following this page was uploaded by [Yu-Cheng Chang ](https://www.researchgate.net/profile/Yu-Cheng-Chang-14?enrichId=rgreq-cf564602e809bef2c75242c2d4328276-XXX&enrichSource=Y292ZXJQYWdlOzM3MjI0MDkzMDtBUzoxMTQzMTI4MTIwMDc0ODcwNEAxNjk4MDcyNTQ5MDgy&el=1_x_10&_esc=publicationCoverPdf)on 23 October 2023.

The user has requested enhancement of the downloaded file.

Based-on Computer Vision Applications for

Bus Stop Passenger Detection System

Yu-Cheng Chang  Hua-Wen Tsai  Chao-Yi Huang                   *Department of Computer Information  Department of Computer Information  Department of Bachelor's Program in and Network Engineering,   and Network Engineering,   Intelligent Robot,*

*Lunghwa University of Science and  Lunghwa University of Science and  National Yunlin University of Science Technology  Technology  and Technology*

Taiwan  Taiwan  Taiwan             yucheng208@outlook.com  hwtsai@mail.lhu.edu.tw  [joyatnet0@gmail.com ](mailto:joyatnet0@gmail.com)

Zong-Rong Wu

*Department of Technical and Vocational Program in Artificial Intelligence,       National Yunlin University of Science and Technology*

Taiwan                  [jason.jiong.long@gmail.com ](mailto:ason.jiong.long@gmail.com)

***Abstract‚Äî*Passenger transport is one of the most common ways of commuting in Taiwan. It plays an important role in the transportation system due to its large number of stations, dense frequency,  and  cheap  transportation.  Due  to  the  unfriendly transportation environment and a large number of passengers, a blind spot of passenger transportation exists, which leads to traffic accidents at the station. We research to make the "Bus Stop Passenger Detection System". Taking the object detection of "Wheelchairs" into consideration, it is more convenient to assist the disabled to find the passenger transportation system, which makes Taiwan's transportation system more convenient.***

***Keywords‚ÄîPose Detection, Object Detection, Transportation System, Wheelchairs***

1. INTRODUCTION

Buses are one of the main modes of transportation in Taiwan's public transportation system due to their low cost, frequent schedules, and dense network of bus stops. However, issues such as aging bus drivers, poor road conditions, and overcrowded buses have led to poor visibility for drivers and an inability to detect passengers waiting at bus stops. This results in passengers being unintentionally passed by as well as posing a potential risk of traffic accidents.

Therefore, many scholars have focused on researching advanced  driver  assistance  systems  (ADAS)  to  achieve a more sophisticated transportation infrastructure. According to  Ref.  [1],  today's  ADAS-equipped  vehicles  use  image recognition and sensors to assist drivers with tasks such as lane departure warning, lane change detection, and collision detection.  Vehicles  integrate  LIDAR  sensors  for  more precise  assistance.  However,  the  application  of  these technologies in public transportation systems is  still rare. Therefore, we design a more suitable assistance system for public transportation from this perspective.

When designing an assistance system suitable for public transportation,  the  driving  environment  and  the  driver's condition must be taken into consideration. Therefore, we found a solution that uses an artificial neural network to build a detection model for human behavior [2]. We extend the detection object to passengers in the bus station to assist in the transportation status of buses.

We  have  proposed  a  Based-on  Computer  Vision Application for Bus Stop Passenger Detection System. This system not only assists drivers in detecting the posture of passengers waiting at the bus stop but also detects whether passengers require mobility aids such as wheelchairs, and provides reminders to drivers through voice functions. In this article, we explain the object detection function of the system, the pose detection and definition, and describe the hardware control scheme in Section II. In Section III, we summarize the content of this study.

2. COMPUTER VISION APPLICATION FOR BUS STOP PASSENGER DETECTION SYSTEM

Figure 1 shows the proposed system with three major parts: object detection, pose detection, and hardware control. We  use  Raspberry  Pi's  Bluetooth  detection  and  pairing function to confirm the distance between the bus and the bus stop. When Bluetooth pairing is successful and the distance is  confirmed,  the  system  calls  the  OpenCV  function  to capture the image in front of the bus. Then, the image is transmitted  to  the  object  detection  module,  and  various objects are labeled. Subsequently, the image is transmitted to the  pose  detection  module  which  uses  joint  labeling  and coordinates  for  deep  learning  to  determine  whether  the passenger's posture meets the defined standard. Finally, the system integrates the results of object detection and pose detection for judgment and output via hardware.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.010.png)

Fig.  1.  Hardware  components  of  computer  vision  application  bus  stop passenger detection system.

1. *Object Detection*

In the system architecture, we use YOLO as the object detector to detect passengers and wheelchairs [3]. Before using the object detection model, we obtained basic training, testing, and validation set data through field shooting and assistance  from  the  Kaggle  image  dataset  blog  [4].  To improve the system and model, we also used web crawling techniques  to  collect  and  select  images  of  people  and wheelchairs  from  the  Internet  [5,6].  Furthermore,  we classified  highly  similar  vehicles  such  as  bicycles  and skateboards into the non-wheelchair category, and conducted multiple experiments to solve the overfitting problem. We found  that  setting  the  number  of  epochs  to  100  during training was the most appropriate, and the convergence effect became optimal.

During  the  experiment,  we  conducted  multiple  tests simulating single and multiple scenarios for both regular and wheelchair passengers. From the output results of YOLO (Fig. 2), we found that both the hand-raising and phone-using actions of regular passengers could be accurately detected and masked.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.011.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.012.png)

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.013.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.014.png)

Fig. 2. YOLO for the scenario of regular single passenger standing and raising hand.

Figure  3  showed  that  wheelchair  passengers  could  be recognized as humans by the detector, regardless of whether they raised their hands or not. In image recognition, simpler environments  improve  recognition  accuracy.  However,  to verify whether multiple persons or objects nearby would lead to  non-wheelchair  detection,  we  conducted  relevant experiments as presented in Fig. 4, which showed that both multiple  regular  passengers  and  a  mix  of  regular  and wheelchair  passengers  successfully  performed  object detection with good performance.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.015.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.016.png)

Fig. 3. Scenario for wheelchaired passenger raising a hand or sitting.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.017.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.018.png)

Fig. 4. Scenario of multiple passengers in a wheelchair and standing.

2. *Pose Detection*

For pose detection, we used OpenPose as the posture detection system [7‚àí9]. After completing the object detection with YOLO, we proceeded with posture detection. During posture detection, we defined three postures: sitting, standing, and raising hand. By training the Artificial Neural Network (ANN) with the COCO 16 key points in the OpenPose model, we completed the training of the ANN model [10]. Next, we integrated the trained ANN model with OpenPose and used the neural network to fully automate the detection results, eliminating the need for developers to perform coordinate calculations and judgments.

In Fig. 5, we input the image detected by YOLO into OpenPose. We detected if the object was in a wheelchair, then determined if it was in a seated position. Next, we used the code in Fig. 6 to integrate the results and enable the system to differentiate between regular passengers waiting at the bus stop and wheelchair users who require assistance. Additionally,  regular  passengers  waited  in  a  standing position,  so  if  a  person  was  detected  by  YOLO,  and OpenPose detected a raised hand, a notification was sent to the bus driver to stop at the bus stop as shown in Figs 7 and 8.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.019.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.020.png)

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.021.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.022.png)

Fig  5.  YOLO  and  OpenPose  for  regular  single  passenger  standing  and raising a hand.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.023.png)

Fig. 6. Code snippet: Integration and decision-making using YOLO and OpenPose.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.024.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.025.png)

Fig 7. YOLO and OpenPose for a wheelchair user raising a hand or sitting.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.026.png) ![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.027.png)

Fig 8. YOLO for multiple passengers in a wheelchair and non-wheelchair.

3. *Hardware Controlled*

Raspberry Pi was chosen as the central processor mainly due to its characteristics and features. Apart from the built-in Bluetooth and Wi-Fi capabilities, it has independent artificial

intelligence  and  logical  computing  abilities,  as  well  as powerful  and  versatile  I/O  ports.  Its  potential  for  future expansion is limitless (Fig. 9).

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.028.png)

2. Yingzi  Lin,  P.  Tang,  Wenjun  Zhang,  ‚ÄúArtificial  neural  network modeling  of  driver  handling  behavior  in  a  driver-vehicle- environment  system,‚Äù  *International  Journal  of  Vehicle  Design*, January 2005.
2. Norhan Bayomi, Mohanned El Kholy, John E. Fernandez, Senem Velipasalar,  Tarek  Rakha,  ‚ÄúBuilding  Envelope  Object  Detection Using  YOLO  Models,‚Äù  *2022  Annual  Modeling  and  Simulation Conference (ANNSIM)*, San Diego, CA, USA, July 2022.
2. D.  Sculley,  Jeff  Moser,  William Cukierski,  Jerad Rose, Meghan O‚ÄôConnell,  Myles  O‚ÄôNeill,  et  al.,  ‚ÄúKaggle‚Äù, https://www.kaggle.com/, 2010.
2. Ph.D  Chang,  Chih-Yung  &  Po-Wei  Su,  (2019)  ‚ÄúSmart  Web Generator  based  on  Artificial  Intelligence  and  Web  Crawler Techniques‚Äù,  *Master's  thesis,  Tamkang  University*,  Taiwan, 2019/06/14.
2. Wes McKinney, ‚ÄúPython for Data Analysis, 2nd Edition‚Äù, *O'Reilly Media, Inc*, October 2017.
2. Zao  Cao,  Gines  Hidalgo,  Tomas  Simon,  Shih-En  Wei,  Yaser Shheikh  ‚ÄúOpenPose:  Realtime  Multi-Person  2D  Pose  Estimation using Part Affinity Fields,‚Äù *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol 43, issue 1, pp. 172-186, January 2021.
2. Z. Cao, T. Simon, S. Wei et al., "Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields," *2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)*, Honolulu, HI, pp. 1302-1310, doi:10.1109/CVPR.2017.143, 2017.
2. Lawrence Y. Deng, Xiang Yann Lim, See Sang Ho, ‚ÄúDeveloping A Parser Framework based on OpenPose Skeleton Detection,‚Äù *2021 IEEE  3rd  Eurasia  Conference  on  Biomedical  Engineering, Healthcare  and  Sustainability  (ECBIOS)*,  Tainan,  Taiwan, 2021/05/28.

Fig. 9. Hardware architecture diagram of the proposed system.

After  describing  the  development  board,  we  installed YOLO and OpenPose on the Raspberry Pi for detection, and the results were processed by the hardware. We provided real-time video to the bus driver on a screen and audio in the sound system. The text-to-speech package was "pyttsx3" as shown in Fig. 10. This package directly calls the system sound effects for speech synthesis, so it is not affected by cross-system platform issues and can be used universally.

![](Aspose.Words.c02d1f7e-c843-40e9-b5a7-ff8d8a54cc74.029.png)

Fig. 10. Code snippet: Text-to-Speech (TTS) synthesis.

3. CONCLUSIONS

Buses  are  important  in  Taiwan's  public  transportation system.  Due  to  aging  drivers,  poor  road  conditions,  and overloading,  drivers'  visibility  can  be  affected,  and passengers may accidentally miss their stops. Therefore, we propose  a  "Computer  Vision  Application  for  Bus  Stop Passenger detection system" that detects objects and human postures and provides visual and audio prompts to bus drivers to reduce incidents of passengers missing their stops. Adding more audio-visual effects to remind passengers is planned for the future of the Computer Vision Application for Bus Stop Passenger  Detection  System,  making  the  system  more complete and encompassing the detection of passenger status inside the vehicle. This will contribute to the improvement of Taiwan's public transportation system.
# REFERENCES
[1]  S. Raviteja, R Shanmughasundaram, ‚ÄúAdvanced Driver Assitance System  (ADAS),‚Äù  *2018  Second  International  Conference  on Intelligent Computing and Control Systems (ICICCS)*, India, June 2018.

[View publication stats](https://www.researchgate.net/publication/372240930)
